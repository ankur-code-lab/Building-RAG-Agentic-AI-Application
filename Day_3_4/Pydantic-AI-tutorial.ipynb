{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["gBYfeJ0JFmG8","WXhyl3b9FHYA","7-9Puvi8-svW","7jdAda5PsR_y","yOgUccOwulXt"],"authorship_tag":"ABX9TyMcZ/E0VkOawAxlANWa9OZM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UQ67QxQ78zmY","executionInfo":{"status":"ok","timestamp":1763167223030,"user_tz":-330,"elapsed":1441,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"7a5dc7f2-d06b-4f70-dac1-15d33218b87e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Package(s) not found: pydantic-ai\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip show pydantic-ai"]},{"cell_type":"code","source":["!pip install -U pydantic-ai"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rbdM44oj9MjI","executionInfo":{"status":"ok","timestamp":1763200081774,"user_tz":-330,"elapsed":39381,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"457f01d6-cd99-463b-b88e-fff3bd559e73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pydantic-ai\n","  Downloading pydantic_ai-1.18.0-py3-none-any.whl.metadata (14 kB)\n","Collecting pydantic-ai-slim==1.18.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading pydantic_ai_slim-1.18.0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting genai-prices>=0.0.35 (from pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading genai_prices-0.0.39-py3-none-any.whl.metadata (6.5 kB)\n","Collecting griffe>=1.3.2 (from pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.28.1)\n","Requirement already satisfied: opentelemetry-api>=1.28.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.37.0)\n","Collecting pydantic-graph==1.18.0 (from pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading pydantic_graph-1.18.0-py3-none-any.whl.metadata (3.9 kB)\n","Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.11.10)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.4.2)\n","Collecting ag-ui-protocol>=0.1.8 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading ag_ui_protocol-0.1.10-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: starlette>=0.45.3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.49.3)\n","Collecting anthropic>=0.70.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading anthropic-0.73.0-py3-none-any.whl.metadata (28 kB)\n","Collecting boto3>=1.40.14 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading boto3-1.40.74-py3-none-any.whl.metadata (6.8 kB)\n","Collecting argcomplete>=3.5.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: prompt-toolkit>=3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (3.0.52)\n","Requirement already satisfied: pyperclip>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.11.0)\n","Requirement already satisfied: rich>=13 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (13.9.4)\n","Collecting cohere>=5.18.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading cohere-5.20.0-py3-none-any.whl.metadata (3.4 kB)\n","Collecting pydantic-evals==1.18.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading pydantic_evals-1.18.0-py3-none-any.whl.metadata (7.8 kB)\n","Collecting fastmcp>=2.12.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading fastmcp-2.13.0.2-py3-none-any.whl.metadata (20 kB)\n","Collecting google-genai>=1.50.1 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading google_genai-1.50.1-py3-none-any.whl.metadata (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting groq>=0.25.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading groq-0.34.1-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: huggingface-hub>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.36.0)\n","Collecting logfire>=3.14.1 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading logfire-4.14.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: mcp>=1.12.3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.21.0)\n","Collecting mistralai>=1.9.10 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: openai>=1.107.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.109.1)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (8.5.0)\n","Collecting temporalio==1.19.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading temporalio-1.19.0-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-auth>=2.36.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.38.0)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.32.4)\n","Requirement already satisfied: anyio>=0 in /usr/local/lib/python3.12/dist-packages (from pydantic-evals==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (4.11.0)\n","Collecting logfire-api>=3.14.1 (from pydantic-evals==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading logfire_api-4.14.2-py3-none-any.whl.metadata (972 bytes)\n","Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-evals==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (6.0.3)\n","Collecting nexus-rpc==1.1.0 (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading nexus_rpc-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: protobuf<7.0.0,>=3.20 in /usr/local/lib/python3.12/dist-packages (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (5.29.5)\n","Collecting types-protobuf>=3.20 (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading types_protobuf-6.32.1.20251105-py3-none-any.whl.metadata (2.2 kB)\n","Requirement already satisfied: typing-extensions<5,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (4.15.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.70.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.9.0)\n","Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.70.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.17.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.70.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.12.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.70.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.3.1)\n","Collecting botocore<1.41.0,>=1.40.74 (from boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading botocore-1.40.74-py3-none-any.whl.metadata (5.9 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n","Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.8 kB)\n","Collecting httpx-sse==0.4.0 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.12/dist-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.33.2)\n","Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.22.1)\n","Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: authlib>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.6.5)\n","Collecting cyclopts>=3.0.0 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading cyclopts-4.2.4-py3-none-any.whl.metadata (12 kB)\n","Collecting exceptiongroup>=1.2.2 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n","Collecting jsonschema-path>=0.3.4 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading jsonschema_path-0.3.4-py3-none-any.whl.metadata (4.3 kB)\n","Collecting openapi-pydantic>=0.5.1 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading openapi_pydantic-0.5.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: platformdirs>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (4.5.0)\n","Collecting py-key-value-aio<0.3.0,>=0.2.8 (from py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading py_key_value_aio-0.2.8-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: python-dotenv>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.2.1)\n","Requirement already satisfied: websockets>=15.0.1 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (15.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (4.9.1)\n","Collecting colorama>=0.4 (from griffe>=1.3.2->pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (25.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.2.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (3.13.2)\n","Collecting executing>=2.0.1 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<1.39.0,>=1.35.0 in /usr/local/lib/python3.12/dist-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.37.0)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: opentelemetry-sdk<1.39.0,>=1.35.0 in /usr/local/lib/python3.12/dist-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.37.0)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.59b0-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (4.25.1)\n","Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.12.0)\n","Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.10.1)\n","Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.0.20)\n","Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (3.0.3)\n","Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.38.0)\n","Collecting eval-type-backport>=0.2.0 (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading eval_type_backport-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting invoke<3.0.0,>=2.2.0 (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.9.0.post0)\n","Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (8.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit>=3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.2.14)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.7.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.5.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.19.2)\n","Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib>=1.5.2->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (43.0.3)\n","Requirement already satisfied: attrs>=23.1.0 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=3.0.0->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (25.4.0)\n","Collecting rich-rst<2.0.0,>=1.3.1 (from cyclopts>=3.0.0->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading rich_rst-1.3.2-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (3.23.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.37.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.28.0)\n","Collecting pathable<0.5.0,>=0.4.1 (from jsonschema-path>=0.3.4->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading pathable-0.4.4-py3-none-any.whl.metadata (1.8 kB)\n","Collecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.1.2)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.39.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.72.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.39.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.37.0)\n","Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.39.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.37.0)\n","Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n","Collecting opentelemetry-api>=1.28.0 (from pydantic-ai-slim==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-util-http==0.59b0 (from opentelemetry-instrumentation-httpx>=0.42b0->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_util_http-0.59b0-py3-none-any.whl.metadata (2.6 kB)\n","INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.58b0-py3-none-any.whl.metadata (7.3 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl.metadata (7.1 kB)\n","INFO: pip is looking at multiple versions of opentelemetry-instrumentation-httpx to determine which version is compatible with other requirements. This could take a while.\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.57b0-py3-none-any.whl.metadata (7.3 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl.metadata (6.7 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.56b0-py3-none-any.whl.metadata (7.3 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.56b0-py3-none-any.whl.metadata (6.7 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.55b1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.55b0-py3-none-any.whl.metadata (7.3 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.55b0-py3-none-any.whl.metadata (6.7 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.54b1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl.metadata (6.8 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.54b0-py3-none-any.whl.metadata (7.4 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.54b0-py3-none-any.whl.metadata (6.8 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.53b1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n","INFO: pip is still looking at multiple versions of opentelemetry-instrumentation-httpx to determine which version is compatible with other requirements. This could take a while.\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.53b0-py3-none-any.whl.metadata (7.3 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.53b0-py3-none-any.whl.metadata (6.8 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.52b1-py3-none-any.whl.metadata (7.1 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.52b0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.52b0-py3-none-any.whl.metadata (6.8 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.51b0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.50b0-py3-none-any.whl.metadata (7.0 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.49b2-py3-none-any.whl.metadata (7.0 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.49b1-py3-none-any.whl.metadata (7.0 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl.metadata (6.2 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.49b0-py3-none-any.whl.metadata (7.0 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl.metadata (6.2 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.48b0-py3-none-any.whl.metadata (7.0 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.47b0-py3-none-any.whl.metadata (7.0 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.46b0-py3-none-any.whl.metadata (6.9 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.45b0-py3-none-any.whl.metadata (6.9 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.44b0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.43b0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\n","Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation_httpx-0.42b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.58b0)\n","Collecting opentelemetry-util-http==0.58b0 (from opentelemetry-instrumentation-httpx>=0.42b0->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading opentelemetry_util_http-0.58b0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: beartype>=0.22.2 in /usr/local/lib/python3.12/dist-packages (from py-key-value-aio<0.3.0,>=0.2.8->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.22.5)\n","Collecting py-key-value-shared==0.2.8 (from py-key-value-aio<0.3.0,>=0.2.8->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading py_key_value_shared-0.2.8-py3-none-any.whl.metadata (682 bytes)\n","Collecting diskcache>=5.6.0 (from py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Collecting pathvalidate>=3.3.1 (from py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: keyring>=25.6.0 in /usr/local/lib/python3.12/dist-packages (from py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (25.6.0)\n","INFO: pip is looking at multiple versions of py-key-value-aio[disk,keyring,memory] to determine which version is compatible with other requirements. This could take a while.\n","Collecting google-auth>=2.36.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n","Collecting cachetools<7.0,>=2.0.0 (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.6.1)\n","Collecting email-validator>=2.0.0 (from pydantic[email]>=2.11.7->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.17.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (8.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (1.22.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib>=1.5.2->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.0.0)\n","Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai)\n","  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (3.4.1)\n","Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.9.0)\n","Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (3.4.0)\n","Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (4.3.0)\n","Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (6.0.1)\n","Requirement already satisfied: docutils in /usr/local/lib/python3.12/dist-packages (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (0.21.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib>=1.5.2->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (2.23)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.18.0->pydantic-ai) (10.8.0)\n","Downloading pydantic_ai-1.18.0-py3-none-any.whl (11 kB)\n","Downloading pydantic_ai_slim-1.18.0-py3-none-any.whl (402 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.3/402.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_evals-1.18.0-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_graph-1.18.0-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading temporalio-1.19.0-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nexus_rpc-1.1.0-py3-none-any.whl (27 kB)\n","Downloading ag_ui_protocol-0.1.10-py3-none-any.whl (7.9 kB)\n","Downloading anthropic-0.73.0-py3-none-any.whl (367 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.40.74-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cohere-5.20.0-py3-none-any.whl (303 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading fastmcp-2.13.0.2-py3-none-any.whl (367 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.5/367.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading genai_prices-0.0.39-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.1/223.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading py_key_value_shared-0.2.8-py3-none-any.whl (14 kB)\n","Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n","Downloading google_genai-1.50.1-py3-none-any.whl (257 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.3/257.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading griffe-1.15.0-py3-none-any.whl (150 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading groq-0.34.1-py3-none-any.whl (136 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading logfire-4.14.2-py3-none-any.whl (228 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.4/228.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.40.74-py3-none-any.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading cyclopts-4.2.4-py3-none-any.whl (185 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading eval_type_backport-0.3.0-py3-none-any.whl (6.1 kB)\n","Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n","Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n","Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading invoke-2.2.1-py3-none-any.whl (160 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading jsonschema_path-0.3.4-py3-none-any.whl (14 kB)\n","Downloading logfire_api-4.14.2-py3-none-any.whl (95 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openapi_pydantic-0.5.1-py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl (33 kB)\n","Downloading opentelemetry_instrumentation_httpx-0.58b0-py3-none-any.whl (15 kB)\n","Downloading opentelemetry_util_http-0.58b0-py3-none-any.whl (7.7 kB)\n","Downloading py_key_value_aio-0.2.8-py3-none-any.whl (69 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading types_protobuf-6.32.1.20251105-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n","Downloading pathable-0.4.4-py3-none-any.whl (9.6 kB)\n","Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n","Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n","Downloading rich_rst-1.3.2-py3-none-any.whl (12 kB)\n","Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, types-requests, types-protobuf, referencing, py-key-value-shared, pathvalidate, pathable, opentelemetry-util-http, nexus-rpc, logfire-api, jmespath, invoke, httpx-sse, fastavro, executing, exceptiongroup, eval-type-backport, dnspython, diskcache, colorama, cachetools, argcomplete, temporalio, py-key-value-aio, jsonschema-path, griffe, google-auth, email-validator, botocore, s3transfer, rich-rst, pydantic-graph, openapi-pydantic, mistralai, groq, google-genai, genai-prices, anthropic, ag-ui-protocol, pydantic-ai-slim, opentelemetry-instrumentation, cyclopts, cohere, boto3, pydantic-evals, opentelemetry-instrumentation-httpx, logfire, fastmcp, pydantic-ai\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 2.0.1\n","    Uninstalling wrapt-2.0.1:\n","      Successfully uninstalled wrapt-2.0.1\n","  Attempting uninstall: referencing\n","    Found existing installation: referencing 0.37.0\n","    Uninstalling referencing-0.37.0:\n","      Successfully uninstalled referencing-0.37.0\n","  Attempting uninstall: httpx-sse\n","    Found existing installation: httpx-sse 0.4.3\n","    Uninstalling httpx-sse-0.4.3:\n","      Successfully uninstalled httpx-sse-0.4.3\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 5.5.2\n","    Uninstalling cachetools-5.5.2:\n","      Successfully uninstalled cachetools-5.5.2\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 2.38.0\n","    Uninstalling google-auth-2.38.0:\n","      Successfully uninstalled google-auth-2.38.0\n","  Attempting uninstall: google-genai\n","    Found existing installation: google-genai 1.49.0\n","    Uninstalling google-genai-1.49.0:\n","      Successfully uninstalled google-genai-1.49.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.43.0 which is incompatible.\n","google-auth-oauthlib 1.2.3 requires google-auth<2.42.0,>=2.15.0, but you have google-auth 2.43.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed ag-ui-protocol-0.1.10 anthropic-0.73.0 argcomplete-3.6.3 boto3-1.40.74 botocore-1.40.74 cachetools-6.2.2 cohere-5.20.0 colorama-0.4.6 cyclopts-4.2.4 diskcache-5.6.3 dnspython-2.8.0 email-validator-2.3.0 eval-type-backport-0.3.0 exceptiongroup-1.3.0 executing-2.2.1 fastavro-1.12.1 fastmcp-2.13.0.2 genai-prices-0.0.39 google-auth-2.43.0 google-genai-1.50.1 griffe-1.15.0 groq-0.34.1 httpx-sse-0.4.0 invoke-2.2.1 jmespath-1.0.1 jsonschema-path-0.3.4 logfire-4.14.2 logfire-api-4.14.2 mistralai-1.9.11 nexus-rpc-1.1.0 openapi-pydantic-0.5.1 opentelemetry-instrumentation-0.58b0 opentelemetry-instrumentation-httpx-0.58b0 opentelemetry-util-http-0.58b0 pathable-0.4.4 pathvalidate-3.3.1 py-key-value-aio-0.2.8 py-key-value-shared-0.2.8 pydantic-ai-1.18.0 pydantic-ai-slim-1.18.0 pydantic-evals-1.18.0 pydantic-graph-1.18.0 referencing-0.36.2 rich-rst-1.3.2 s3transfer-0.14.0 temporalio-1.19.0 types-protobuf-6.32.1.20251105 types-requests-2.32.4.20250913 wrapt-1.17.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"cbe56e6fe8f941a4af5fef619fe12e76"}},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"],"metadata":{"id":"yh_Xbkve90ax"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Use Groq as LLM provider in Pydantic-AI\n","\n","https://ai.pydantic.dev/models/groq/\n","\n","\n","- Agents Introduction in Pydantic-AI\n","\n","https://ai.pydantic.dev/agents/#introduction"],"metadata":{"id":"xZ3vu7U4BtxD"}},{"cell_type":"markdown","source":["# Simple Agent"],"metadata":{"id":"gBYfeJ0JFmG8"}},{"cell_type":"code","source":["from pydantic_ai import Agent\n","\n","agent = Agent('groq:openai/gpt-oss-120b', system_prompt=\"You answer questions only related to sports\")"],"metadata":{"id":"lJkPIhCf-nyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nest_asyncio\n","\n","nest_asyncio.apply()\n","\n","result = agent.run_sync('Where does \"hello world\" come from?')"],"metadata":{"id":"g-b22Aog-5ux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Km1IuTPs8Y5c","executionInfo":{"status":"ok","timestamp":1763200572639,"user_tz":-330,"elapsed":47,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"80a04a2f-3485-4911-d87e-9ab0f11d46c9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AgentRunResult(output='I’m sorry, but I can only help with questions about sports. If you have a sports‑related query, feel free to ask!')"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["result.output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vbSxlvg9-Dgo","executionInfo":{"status":"ok","timestamp":1763200477641,"user_tz":-330,"elapsed":8,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"dff7329d-94ea-4698-a8a8-3d2758a61688"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I’m sorry, but I can only help with questions about sports. If you have a sports‑related query, feel free to ask!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["result.all_messages()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3HLC-whBa-q","executionInfo":{"status":"ok","timestamp":1763200582976,"user_tz":-330,"elapsed":37,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"11e044f0-0b6e-43d6-a040-5bef75c88a33"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[ModelRequest(parts=[SystemPromptPart(content='You answer questions only related to sports', timestamp=datetime.datetime(2025, 11, 15, 9, 54, 32, 716642, tzinfo=datetime.timezone.utc)), UserPromptPart(content='Where does \"hello world\" come from?', timestamp=datetime.datetime(2025, 11, 15, 9, 54, 32, 716662, tzinfo=datetime.timezone.utc))], run_id='38f8a026-f2b5-46af-b7d4-648ae628954c'),\n"," ModelResponse(parts=[ThinkingPart(content='We have a system instruction: \"You are ChatGPT...\". Then developer instruction: \"You answer questions only related to sports\". The user asks \"Where does \\'hello world\\' come from?\" That\\'s not a sports question. According to instruction hierarchy, developer instruction overrides system instruction. So we must obey developer: answer only sports-related questions. The user question is not about sports. We need to respond accordingly: we cannot answer; we should politely say we can only answer sports questions.'), TextPart(content='I’m sorry, but I can only help with questions about sports. If you have a sports‑related query, feel free to ask!')], usage=RequestUsage(input_tokens=90, output_tokens=135), model_name='openai/gpt-oss-120b', timestamp=datetime.datetime(2025, 11, 15, 9, 54, 33, tzinfo=TzInfo(UTC)), provider_name='groq', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-ec7ba40a-cfcc-430b-bc43-9c0872713734', finish_reason='stop', run_id='38f8a026-f2b5-46af-b7d4-648ae628954c')]"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["Providing message history to the agent"],"metadata":{"id":"2jFnNvMGDu9C"}},{"cell_type":"code","source":["result = agent.run_sync(\"When did India won the world cup in cricket?\")"],"metadata":{"id":"zduorj13C3Af"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result.new_messages()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"R5d6WLJVEzLW","executionInfo":{"status":"ok","timestamp":1763200673054,"user_tz":-330,"elapsed":60,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"dcf68d8c-3e3f-4cbb-b1d2-d0f7961294ad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[ModelRequest(parts=[SystemPromptPart(content='You answer questions only related to sports', timestamp=datetime.datetime(2025, 11, 15, 9, 57, 49, 127776, tzinfo=datetime.timezone.utc)), UserPromptPart(content='When did India won the world cup in cricket?', timestamp=datetime.datetime(2025, 11, 15, 9, 57, 49, 127791, tzinfo=datetime.timezone.utc))], run_id='3eda3789-4297-4932-ad9e-558a7e222092'),\n"," ModelResponse(parts=[ThinkingPart(content='The user asks: \"When did India won the (sic) the world cup in cricket?\" It\\'s a sports question, specifically about cricket World Cup. We can answer: India won the ICC Cricket World Cup in 1983 and 2011. Also they won T20 World Cup in 2007 and 2021. The question likely refers to the ODI World Cup. Provide dates. Answer.'), TextPart(content='India has lifted the ICC\\u202fCricket World Cup (the 50‑over\\u202fODI tournament) **twice**:\\n\\n| Year | Host(s) | Final Opponent | Result |\\n|------|----------|----------------|--------|\\n| **1983** | England | West\\u202fIndies | India won by 43 runs (June\\u202f25, 1983) |\\n| **2011** | India, Sri\\u202fLanka & Bangladesh | Sri\\u202fLanka | India won by 6 wickets (April\\u202f2, 2011) |\\n\\nIn addition, India has won the ICC\\u202fT20\\u202fWorld Cup twice (2007\\u202fand\\u202f2021), but those are separate from the 50‑over World Cup.')], usage=RequestUsage(input_tokens=91, output_tokens=241), model_name='openai/gpt-oss-120b', timestamp=datetime.datetime(2025, 11, 15, 9, 57, 49, tzinfo=TzInfo(UTC)), provider_name='groq', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-07f0afae-9479-42ec-bf9e-37b0f0cc4a01', finish_reason='stop', run_id='3eda3789-4297-4932-ad9e-558a7e222092')]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["agent.run_sync(\"What was my last question?\", message_history = result.new_messages())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLfCRqKtDpSt","executionInfo":{"status":"ok","timestamp":1763200698721,"user_tz":-330,"elapsed":307,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"ef4ae826-cbe6-44a9-af51-3337f0aaffea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AgentRunResult(output='Your most recent question was: **“When did India won the world cup in cricket?”**')"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["# Agent with Structured Response"],"metadata":{"id":"WXhyl3b9FHYA"}},{"cell_type":"markdown","source":["\n","This example shows how to get structured, type-safe responses from the agent.\n","\n","Key concepts:\n","- Using Pydantic models to define response structure\n","- Type validation and safety\n"],"metadata":{"id":"AMgpkwToFsI2"}},{"cell_type":"code","source":["from pydantic import BaseModel, Field\n","\n","model = 'groq:openai/gpt-oss-120b'\n","\n","class ResponseModel(BaseModel):\n","    \"\"\"Structured response with metadata.\"\"\"\n","\n","    response: str\n","    needs_escalation: bool\n","    follow_up_required: bool\n","    sentiment: str = Field(description=\"Customer sentiment analysis\")\n","\n","\n","agent2 = Agent(\n","    model=model,\n","    output_type=ResponseModel,\n","    system_prompt=(\n","        \"You are an intelligent customer support agent. \"\n","        \"Analyze queries carefully and provide structured responses.\"\n","    ),\n",")\n","\n","response = agent2.run_sync(\"How can I track my order #12345?\")\n","response.output.model_dump_json(indent=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"PpcVL-XeElxZ","executionInfo":{"status":"ok","timestamp":1763200976609,"user_tz":-330,"elapsed":784,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"0020096b-32ed-476b-95ff-4f393e7e3b97"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'{\\n  \"response\": \"To track your order #12345, please follow these steps:\\\\n1. Visit our website and log into your account.\\\\n2. Go to the \\\\\"My Orders\\\\\" section.\\\\n3. Locate order #12345 in the list and click the \\\\\"Track\\\\\" button.\\\\n4. You’ll see the latest shipping status and an estimated delivery date.\\\\n\\\\nIf you prefer, you can also track your order directly using this link: https://www.example.com/track?order=12345 (replace with the actual tracking URL if available).\\\\n\\\\nIf you encounter any issues or the status isn’t updating, feel free to reply to this message or contact our support team at support@example.com or call 1‑800‑123‑4567.\",\\n  \"needs_escalation\": false,\\n  \"follow_up_required\": false,\\n  \"sentiment\": \"neutral\"\\n}'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["print(response.output.response)"],"metadata":{"id":"YvcjYMOVG1z9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763201010435,"user_tz":-330,"elapsed":20,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"e0912c2e-5ea0-4fe4-ae18-280de3d0233f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["To track your order #12345, please follow these steps:\n","1. Visit our website and log into your account.\n","2. Go to the \"My Orders\" section.\n","3. Locate order #12345 in the list and click the \"Track\" button.\n","4. You’ll see the latest shipping status and an estimated delivery date.\n","\n","If you prefer, you can also track your order directly using this link: https://www.example.com/track?order=12345 (replace with the actual tracking URL if available).\n","\n","If you encounter any issues or the status isn’t updating, feel free to reply to this message or contact our support team at support@example.com or call 1‑800‑123‑4567.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TmTMpvYf-fMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RAG"],"metadata":{"id":"7-9Puvi8-svW"}},{"cell_type":"code","source":["import os\n","os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"],"metadata":{"id":"Pt3ug53cBrKf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pydantic_ai import Agent\n","# from pydantic_ai.models.groq import GroqModel\n","from typing import List\n","\n","# 1. Choose your model (OpenAI example; adjust to what you use)\n","# model = GroqModel(\"openai:gpt-oss-120b\")\n","\n","# 2. Define a retriever tool for RAG\n","\n","def retrieve_docs(query: str) -> List[str]:\n","    \"\"\"\n","    Retrieve relevant documents for a query.\n","    In real life, call your vector DB / search index here.\n","    \"\"\"\n","    # TODO: replace this with your real retrieval\n","    fake_corpus = {\n","        \"pydantic\": \"Pydantic is a library for data validation using Python type hints.\",\n","        \"rag\": \"RAG stands for Retrieval Augmented Generation.\",\n","        \"agent\": \"Agents can call tools to fetch external information.\"\n","    }\n","    return [text for key, text in fake_corpus.items() if key in query.lower()]\n","\n","# 3. Create the agent and attach the tool\n","rag_agent = Agent(\n","    \"groq:openai/gpt-oss-120b\",\n","    system_prompt=(\n","        \"You are a RAG assistant.\\n\"\n","        \"- Use the `retrieve_docs` tool whenever user questions may require external info.\\n\"\n","        \"- When you call it, read the returned documents and answer using ONLY that info plus the question.\\n\"\n","        \"- If the tool returns nothing, say you couldn't find anything relevant.\"\n","    ),\n","    tools=[retrieve_docs],\n",")\n","\n","# 4. Run a RAG-style query\n","async def ask(question: str):\n","    result = await rag_agent.run(\n","        question,\n","        # (optional) you can pass metadata, user id, etc. here\n","    )\n","    print(result)\n","    # print(result.data)   # final answer text\n","    # print(result.tool_calls)  # how it used tools, if you want to debug\n"],"metadata":{"id":"___Vh8pJ-gYi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import asyncio\n","\n","result = asyncio.run(ask(\"Explain what RAG is and how it relates to agents.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KP_oeipZ-gUs","executionInfo":{"status":"ok","timestamp":1763202172423,"user_tz":-330,"elapsed":1223,"user":{"displayName":"ankur saxena","userId":"11041009981365514655"}},"outputId":"cf407723-632c-4a7e-e49a-0cdd202f3692"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AgentRunResult(output='RAG stands for **Retrieval‑Augmented Generation**.\\u202fIt is a technique where a language model doesn’t rely solely on its internal knowledge; instead, it first pulls in relevant external information and then uses that retrieved content to generate its response.\\n\\nAgents—software entities that can execute actions on behalf of a user—often have the ability to call tools that fetch external data (e.g., search APIs, databases, or document stores). When an agent uses such a tool to retrieve information before producing an answer, it is effectively performing the “retrieval” step of RAG. The subsequent generation step then incorporates the fetched data, completing the Retrieval‑Augmented Generation cycle. In short, agents provide the mechanism (tool calls) that enables the retrieval part of RAG, allowing the model to produce more up‑to‑date and accurate responses.')\n"]}]},{"cell_type":"markdown","source":["# Example: Evaluating a RAG application (retrieval + answer quality)\n","\n","Here we combine:\n","\n","A Pydantic-AI RAG agent using a search_docs tool (very similar to the official Pydantic RAG example).\n","\n","\n","Pydantic-Evals to:\n","\n","1. Check that the answer is grounded in retrieved docs\n","\n","2. Check that the answer is relevant to the question"],"metadata":{"id":"7jdAda5PsR_y"}},{"cell_type":"markdown","source":["- RAG agent skeleton\n","\n","Assume you already have an in-memory document store and a simple retrieval function."],"metadata":{"id":"xF_o38PRshlu"}},{"cell_type":"code","source":["from dataclasses import dataclass\n","from typing import List\n","from pydantic import BaseModel\n","from pydantic_ai import Agent, RunContext\n","\n","# --- Domain models ----------------------------------------------------\n","\n","class DocChunk(BaseModel):\n","    id: str\n","    text: str\n","\n","@dataclass\n","class RAGDeps:\n","    # could be a vector store or just a list of docs in a PoC\n","    documents: List[DocChunk]\n","\n","# --- Retrieval tool ---------------------------------------------------\n","\n","def retrieve_relevant_chunks(query: str, docs: List[DocChunk], k: int = 3) -> List[DocChunk]:\n","    # toy implementation: top-k by simple keyword overlap\n","    scores = []\n","    query_terms = set(query.lower().split())\n","    for d in docs:\n","        overlap = len(query_terms & set(d.text.lower().split()))\n","        scores.append((overlap, d))\n","    scores.sort(reverse=True, key=lambda t: t[0])\n","    return [d for score, d in scores[:k] if score > 0]\n","\n","# --- RAG agent --------------------------------------------------------\n","\n","class RAGAnswer(BaseModel):\n","    answer: str\n","    used_doc_ids: List[str]\n","\n","rag_agent = Agent[RAGDeps, RAGAnswer](\n","    \"openai:gpt-4o-mini\",\n","    deps_type=RAGDeps,\n","    output_type=RAGAnswer,\n","    instructions=\"\"\"\n","    You are a documentation assistant.\n","    Use ONLY the provided context chunks to answer the question.\n","    If the answer is not in the context, say you don't know.\n","    Return the IDs of the chunks you used.\n","    \"\"\",\n",")\n","\n","@rag_agent.tool  # function tool exposes retrieval to the model\n","def search_docs(ctx: RunContext[RAGDeps], query: str) -> List[DocChunk]:\n","    return retrieve_relevant_chunks(query, ctx.deps.documents)\n"],"metadata":{"collapsed":true,"id":"k_q_Mc7d-gQh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Task function for evals\n","\n","We want the task function to return both answer text and which docs were used:"],"metadata":{"id":"rs44lyf4sutH"}},{"cell_type":"code","source":["from typing import TypedDict, List\n","\n","class RAGOutput(TypedDict):\n","    answer: str\n","    used_doc_ids: List[str]\n","\n","def rag_task(question: str, deps: RAGDeps) -> RAGOutput:\n","    result = rag_agent.run_sync(question, deps=deps)\n","    out = result.output\n","    return {\"answer\": out.answer, \"used_doc_ids\": out.used_doc_ids}\n"],"metadata":{"id":"-5_pe1dKCd8D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For simple evals we can partially “fix” dependencies (e.g., same KB for all cases):"],"metadata":{"id":"OGcKg-vUtepP"}},{"cell_type":"code","source":["# Partial application to match the expected signature inputs -> output\n","my_docs = [\n","    DocChunk(id=\"d1\", text=\"Our refund policy allows returns within 30 days.\"),\n","    DocChunk(id=\"d2\", text=\"Tech support is available 24/7 via chat.\"),\n","]\n","\n","def rag_task_fixed(question: str) -> RAGOutput:\n","    return rag_task(question, deps=RAGDeps(documents=my_docs))\n"],"metadata":{"id":"lSPhwPdXtgMP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Define RAG-specific eval dataset\n","\n","Key idea: in each case we specify both:\n","\n","1. The question\n","\n","2. The expected supporting doc IDs\n","\n","3. Optionally, a reference answer"],"metadata":{"id":"9bVrJyhvs6Dd"}},{"cell_type":"code","source":["from pydantic_evals import Case, Dataset\n","from pydantic_evals.evaluators import LLMJudge\n","\n","class RAGInputs(BaseModel):\n","    question: str\n","    expected_doc_ids: List[str]\n","    reference_answer: str\n","\n","dataset = Dataset[RAGInputs, RAGOutput](\n","    cases=[\n","        Case(\n","            name=\"refund_policy\",\n","            inputs=RAGInputs(\n","                question=\"What is your refund policy?\",\n","                expected_doc_ids=[\"d1\"],\n","                reference_answer=\"We allow refunds within 30 days of purchase.\",\n","            ),\n","        ),\n","        Case(\n","            name=\"support_hours\",\n","            inputs=RAGInputs(\n","                question=\"When is tech support available?\",\n","                expected_doc_ids=[\"d2\"],\n","                reference_answer=\"Tech support is available 24/7 via chat.\",\n","            ),\n","        ),\n","    ],\n",")\n"],"metadata":{"id":"JEvHz2J5s_-Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Add custom evaluators: retrieval + groundedness\n","\n","You can layer:\n","\n","Retrieval precision: Did the agent use the right doc IDs?\n","\n","Groundedness: Is the answer consistent with provided docs?\n","(Good use case for LLMJudge.)\n"],"metadata":{"id":"eB9EN8PstFcj"}},{"cell_type":"code","source":["### Retrieval evaluator (deterministic)\n","\n","\n","from dataclasses import dataclass\n","from pydantic_evals.evaluators import Evaluator, EvaluatorContext, EvaluationReason\n","\n","@dataclass\n","class RetrievalMatch(Evaluator[RAGInputs, RAGOutput, None]):\n","    \"\"\"Check that the agent used all expected docs.\"\"\"\n","\n","    def evaluate(self, ctx: EvaluatorContext[RAGInputs, RAGOutput, None]):\n","        expected = set(ctx.inputs.expected_doc_ids)\n","        used = set(ctx.output[\"used_doc_ids\"])\n","        missing = expected - used\n","        extra = used - expected\n","\n","        ok = not missing  # require at least all expected docs\n","        reason = f\"missing={missing}, extra={extra}\"\n","        return EvaluationReason(value=ok, reason=reason)\n"],"metadata":{"id":"duh2EIC1tM14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attach this evaluator to the dataset:"],"metadata":{"id":"jhiTrw5Rt2uW"}},{"cell_type":"code","source":["dataset.add_evaluator(RetrievalMatch())"],"metadata":{"id":"kG1F5wXAt3i3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(You can also attach evaluators to specific cases by specific_case=\"refund_policy\".)"],"metadata":{"id":"YFrQgYqBt9TJ"}},{"cell_type":"code","source":["### Grounded answer evaluator with LLMJudge\n","\n","\n","from pydantic_evals.evaluators import LLMJudge\n","\n","groundedness_rubric = \"\"\"\n","You are evaluating an answer produced by a RAG system.\n","\n","Criteria (True vs False):\n","- TRUE if the answer is fully supported by the provided reference_answer text\n","  and does not introduce any contradictions or extra facts.\n","- FALSE if the answer contradicts, fabricates, or goes beyond the reference.\n","\"\"\"\n","\n","dataset.add_evaluator(\n","    LLMJudge(\n","        rubric=groundedness_rubric,\n","        include_input=True,            # see the question + inputs\n","        include_expected_output=True,  # see reference_answer\n","        # model optional – uses default judge model, typically `openai:gpt-4o`\n","    )\n",")\n","\n","\n","### Under the hood, LLMJudge will call a judge model and return a boolean/score with an explanation."],"metadata":{"id":"Z2dRB5s7t96P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run RAG evaluation"],"metadata":{"id":"ke0fX--0uKW0"}},{"cell_type":"code","source":["report = dataset.evaluate_sync(rag_task_fixed)\n","report.print()"],"metadata":{"id":"Yq6axVNIuKGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Example: Evaluating a Agent with Pydantic-Evals"],"metadata":{"id":"yOgUccOwulXt"}},{"cell_type":"markdown","source":["- Agent we want to evaluate\n","\n","A minimal Pydantic-AI agent that classifies user queries into support intents:"],"metadata":{"id":"YXeAbkq8utbT"}},{"cell_type":"code","source":["from typing import Literal\n","from pydantic import BaseModel\n","from pydantic_ai import Agent\n","\n","class IntentOutput(BaseModel):\n","    intent: Literal[\"refund\", \"technical_support\", \"sales\", \"other\"]\n","    reasoning: str\n","\n","# Pydantic AI agent: takes a user query and returns typed output\n","intent_agent = Agent[None, IntentOutput](\n","    \"openai:gpt-4o-mini\",  # or any configured model\n","    instructions=\"\"\"\n","    You are a support triage bot.\n","    Classify the user's message into one of: refund, technical_support, sales, other.\n","    Explain your reasoning in one or two sentences.\n","    \"\"\",\n","    output_type=IntentOutput,\n",")\n"],"metadata":{"id":"0CNv_qUEusvO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is straight out of typical Pydantic-AI usage: agents are parameterized by dependency type and output model, and output_type is a Pydantic model that gets validated\n","\n","\n","We’ll now evaluate whether the classification is correct across a dataset."],"metadata":{"id":"1dF_uX2RvNvH"}},{"cell_type":"markdown","source":["- Wrap the agent as a task function\n","\n","Pydantic-Evals expects a function to call for each test case. We wrap the agent:"],"metadata":{"id":"PDYYfSn6vYYa"}},{"cell_type":"code","source":["def classify_intent_task(user_message: str) -> str:\n","    \"\"\"Task function for evals – returns only the intent label.\"\"\"\n","    result = intent_agent.run_sync(user_message)\n","    return result.output.intent\n"],"metadata":{"id":"JypwBdz-vV6V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Define dataset + evaluators (deterministic + LLM-judge)"],"metadata":{"id":"l2z2c-bGveNk"}},{"cell_type":"code","source":["from pydantic_evals import Case, Dataset\n","from pydantic_evals.evaluators import EqualsExpected, LLMJudge\n","\n","# 1) Dataset with labeled cases\n","dataset = Dataset[str, str](\n","    cases=[\n","        Case(\n","            name=\"refund_request\",\n","            inputs=\"I want a refund for the headphones I bought last week.\",\n","            expected_output=\"refund\",\n","        ),\n","        Case(\n","            name=\"login_issue\",\n","            inputs=\"I can't log into my account, it keeps saying password invalid.\",\n","            expected_output=\"technical_support\",\n","        ),\n","        Case(\n","            name=\"pricing_question\",\n","            inputs=\"Do you offer any discount if we buy 100 licenses?\",\n","            expected_output=\"sales\",\n","        ),\n","    ],\n","    evaluators=[\n","        # Exact correctness: output == expected_output\n","        EqualsExpected(),\n","        # Subjective check: was the classification reasonable?\n","        LLMJudge(\n","            rubric=(\n","                \"Judge if the predicted intent label is a reasonable \"\n","                \"classification for the given user message. \"\n","                \"Return true only if it clearly fits.\"\n","            ),\n","            include_input=True,\n","            include_expected_output=True,\n","            # model optional – uses default judge model if omitted\n","        ),\n","    ],\n",")\n"],"metadata":{"id":"KPW6-JhGvfq9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- EqualsExpected is a built-in comparison evaluator.\n","\n","\n","- LLMJudge is an LLM-as-a-judge evaluator for subjective criteria (correctness of label given context)."],"metadata":{"id":"DH3pj1DHvkoM"}},{"cell_type":"code","source":["### Run the evaluation\n","\n","\n","report = dataset.evaluate_sync(classify_intent_task)\n","\n","# Pretty print high-level summary in training session:\n","report.print()\n"],"metadata":{"id":"5YFq_ZZyvpSe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pydantic-Evals’ data model is:\n","\n","Dataset – list of Cases + Evaluators\n","\n","Experiment – running dataset.evaluate(task)\n","\n","EvaluationReport – structured result with scores, assertions, durations, etc."],"metadata":{"id":"QAtTkwXAvutH"}}]}
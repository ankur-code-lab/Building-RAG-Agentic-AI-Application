{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WXhyl3b9FHYA",
        "7jdAda5PsR_y",
        "yOgUccOwulXt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4e655c6fff194322b4ec1abba16afa99": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e20229f8022c4b73a5eab001fa34b0d8",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Evaluating 1 test case(s) in parallel \u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[38;2;0;229;255m  0%\u001b[0m \u001b[38;2;87;3;255m0:00:00\u001b[0m\n    ğŸ¯ Evaluating test case #0        \u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[38;2;0;229;255m  0%\u001b[0m \u001b[38;2;87;3;255m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating 1 test case(s) in parallel <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #00e5ff; text-decoration-color: #00e5ff\">  0%</span> <span style=\"color: #5703ff; text-decoration-color: #5703ff\">0:00:00</span>\n    ğŸ¯ Evaluating test case #0        <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #00e5ff; text-decoration-color: #00e5ff\">  0%</span> <span style=\"color: #5703ff; text-decoration-color: #5703ff\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "e20229f8022c4b73a5eab001fa34b0d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ67QxQ78zmY",
        "outputId": "6965c8c1-0561-4ca8-ecac-ec6add252c32",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: pydantic-ai\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip show pydantic-ai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pydantic-ai"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbdM44oj9MjI",
        "outputId": "8b45b23e-8ae2-4d9c-f1f8-e28d0aa8faa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic-ai in /usr/local/lib/python3.12/dist-packages (1.28.0)\n",
            "Requirement already satisfied: pydantic-ai-slim==1.28.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.28.0)\n",
            "Requirement already satisfied: genai-prices>=0.0.40 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.0.47)\n",
            "Requirement already satisfied: griffe>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.15.0)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.28.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.28.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.39.0)\n",
            "Requirement already satisfied: pydantic-graph==1.28.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.28.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.12.3)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.4.2)\n",
            "Requirement already satisfied: ag-ui-protocol>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.1.10)\n",
            "Requirement already satisfied: starlette>=0.45.3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.48.0)\n",
            "Requirement already satisfied: anthropic>=0.75.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.75.0)\n",
            "Requirement already satisfied: boto3>=1.40.14 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.42.5)\n",
            "Requirement already satisfied: argcomplete>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (3.6.3)\n",
            "Requirement already satisfied: prompt-toolkit>=3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (3.0.52)\n",
            "Requirement already satisfied: pyperclip>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.11.0)\n",
            "Requirement already satisfied: rich>=13 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (13.9.4)\n",
            "Requirement already satisfied: cohere>=5.18.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (5.20.0)\n",
            "Requirement already satisfied: pydantic-evals==1.28.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.28.0)\n",
            "Requirement already satisfied: fastmcp>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.13.3)\n",
            "Requirement already satisfied: google-genai>=1.51.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.53.0)\n",
            "Requirement already satisfied: groq>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.37.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.36.0)\n",
            "Requirement already satisfied: logfire>=3.14.1 in /usr/local/lib/python3.12/dist-packages (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.16.0)\n",
            "Requirement already satisfied: mcp>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.22.0)\n",
            "Requirement already satisfied: mistralai>=1.9.10 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.9.11)\n",
            "Requirement already satisfied: openai>=1.107.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.8.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (9.1.2)\n",
            "Requirement already satisfied: temporalio==1.19.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.19.0)\n",
            "Requirement already satisfied: google-auth>=2.36.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.43.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.32.4)\n",
            "Requirement already satisfied: anyio>=0 in /usr/local/lib/python3.12/dist-packages (from pydantic-evals==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.12.0)\n",
            "Requirement already satisfied: logfire-api>=3.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic-evals==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.16.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-evals==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (6.0.3)\n",
            "Requirement already satisfied: nexus-rpc==1.1.0 in /usr/local/lib/python3.12/dist-packages (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.1.0)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=3.20 in /usr/local/lib/python3.12/dist-packages (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (5.29.5)\n",
            "Requirement already satisfied: types-protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (6.32.1.20251105)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.75.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.75.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.75.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.75.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.3.1)\n",
            "Requirement already satisfied: botocore<1.43.0,>=1.42.5 in /usr/local/lib/python3.12/dist-packages (from boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.42.5)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.16.0)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.12/dist-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.12.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.12/dist-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.12/dist-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.41.4)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.22.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.32.4.20250913)\n",
            "Requirement already satisfied: authlib>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.6.5)\n",
            "Requirement already satisfied: cyclopts>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.3.1)\n",
            "Requirement already satisfied: jsonschema-path>=0.3.4 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.3.4)\n",
            "Requirement already satisfied: openapi-pydantic>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.5.1)\n",
            "Requirement already satisfied: platformdirs>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.5.0)\n",
            "Requirement already satisfied: py-key-value-aio<0.4.0,>=0.2.8 in /usr/local/lib/python3.12/dist-packages (from py-key-value-aio[disk,memory]<0.4.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.3.0)\n",
            "Requirement already satisfied: python-dotenv>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.2.1)\n",
            "Requirement already satisfied: uvicorn>=0.35 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.38.0)\n",
            "Requirement already satisfied: websockets>=15.0.1 in /usr/local/lib/python3.12/dist-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (15.0.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.9.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe>=1.3.2->pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.4.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.5->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.5->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.5->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.5->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.5->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (3.13.2)\n",
            "Requirement already satisfied: executing>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.2.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<1.40.0,>=1.39.0 in /usr/local/lib/python3.12/dist-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation>=0.41b0 in /usr/local/lib/python3.12/dist-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.60b0)\n",
            "Requirement already satisfied: opentelemetry-sdk<1.40.0,>=1.39.0 in /usr/local/lib/python3.12/dist-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-httpx>=0.42b0 in /usr/local/lib/python3.12/dist-packages (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.60b0)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.12.0)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (3.0.3)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.3.1)\n",
            "Requirement already satisfied: invoke<3.0.0,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (8.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit>=3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.2.14)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.7.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.19.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib>=1.6.5->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (43.0.3)\n",
            "Requirement already satisfied: attrs>=23.1.0 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=4.0.0->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (25.4.0)\n",
            "Requirement already satisfied: rich-rst<2.0.0,>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=4.0.0->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==1.28.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (3.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.30.0)\n",
            "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema-path>=0.3.4->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.4.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.1.2)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.40.0,>=1.39.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.40.0,>=1.39.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.40.0,>=1.39.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.60b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.17.3)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.60b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-httpx>=0.42b0->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.60b0)\n",
            "Requirement already satisfied: py-key-value-shared==0.3.0 in /usr/local/lib/python3.12/dist-packages (from py-key-value-aio<0.4.0,>=0.2.8->py-key-value-aio[disk,memory]<0.4.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.3.0)\n",
            "Requirement already satisfied: beartype>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from py-key-value-aio<0.4.0,>=0.2.8->py-key-value-aio[disk,memory]<0.4.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.22.8)\n",
            "Requirement already satisfied: diskcache>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from py-key-value-aio[disk,memory]<0.4.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (5.6.3)\n",
            "Requirement already satisfied: pathvalidate>=3.3.1 in /usr/local/lib/python3.12/dist-packages (from py-key-value-aio[disk,memory]<0.4.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.6.1)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pydantic[email]>=2.11.7->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.17.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.35->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (8.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]<1.0.0,>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (1.22.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib>=1.6.5->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.0.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.8.0)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.12/dist-packages (from rich-rst<2.0.0,>=1.3.1->cyclopts>=4.0.0->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (0.21.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib>=1.6.5->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.28.0->pydantic-ai) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "yh_Xbkve90ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use Groq as LLM provider in Pydantic-AI\n",
        "\n",
        "https://ai.pydantic.dev/models/groq/\n",
        "\n",
        "\n",
        "- Agents Introduction in Pydantic-AI\n",
        "\n",
        "https://ai.pydantic.dev/agents/#introduction"
      ],
      "metadata": {
        "id": "xZ3vu7U4BtxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Agent"
      ],
      "metadata": {
        "id": "gBYfeJ0JFmG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent"
      ],
      "metadata": {
        "id": "n4ZQ5-CqxjJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "agent = Agent('groq:openai/gpt-oss-120b', system_prompt=\"You answer questions only related to sports\")"
      ],
      "metadata": {
        "id": "lJkPIhCf-nyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await agent.run('Where does \"hello world\" come from?')\n"
      ],
      "metadata": {
        "id": "W6hi_nkXsFTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vbSxlvg9-Dgo",
        "outputId": "56b0713d-c1c1-4760-8da7-30e49865fcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Iâ€™m sorry, but I can only help with questions about sports.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.all_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3HLC-whBa-q",
        "outputId": "0a4f227f-d8ed-48e3-d492-6d799f2db2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ModelRequest(parts=[SystemPromptPart(content='You answer questions only related to sports', timestamp=datetime.datetime(2025, 12, 9, 5, 6, 55, 150523, tzinfo=datetime.timezone.utc)), UserPromptPart(content='Where does \"hello world\" come from?', timestamp=datetime.datetime(2025, 12, 9, 5, 6, 55, 150536, tzinfo=datetime.timezone.utc))], run_id='c1364a1d-990e-4721-a477-4562251c37ed'),\n",
              " ModelResponse(parts=[ThinkingPart(content='The user asks a question unrelated to sports. The developer instruction says \"You answer questions only related to sports\". So we must refuse or give a brief apology and state we can only answer sports-related queries. According to policy, we can politely decline.'), TextPart(content='Iâ€™m sorry, but I can only help with questions about sports.')], usage=RequestUsage(input_tokens=90, output_tokens=74), model_name='openai/gpt-oss-120b', timestamp=datetime.datetime(2025, 12, 9, 5, 6, 55, tzinfo=TzInfo(0)), provider_name='groq', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-d19d413d-97f1-424e-bfe5-b406201cc77c', finish_reason='stop', run_id='c1364a1d-990e-4721-a477-4562251c37ed')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Providing message history to the agent"
      ],
      "metadata": {
        "id": "2jFnNvMGDu9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await agent.run(\"When did India won the world cup in cricket?\")"
      ],
      "metadata": {
        "id": "zduorj13C3Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.new_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R5d6WLJVEzLW",
        "outputId": "ac24da79-30d1-43cc-84c7-ccdf962a12a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ModelRequest(parts=[SystemPromptPart(content='You answer questions only related to sports', timestamp=datetime.datetime(2025, 12, 9, 5, 8, 19, 677422, tzinfo=datetime.timezone.utc)), UserPromptPart(content='When did India won the world cup in cricket?', timestamp=datetime.datetime(2025, 12, 9, 5, 8, 19, 677434, tzinfo=datetime.timezone.utc))], run_id='118711a7-0130-484c-adcd-d1a1c2ac885f'),\n",
              " ModelResponse(parts=[ThinkingPart(content='The user asks: \"When did India won the (sic) the world cup in cricket?\" This is sports related. Provide answer. Provide years: 1983 (won under Kapil Dev), 2011 (under MS Dhoni). Also maybe T20 World Cup 2021, 2022? The question likely about ICC Cricket World Cup (ODI). Provide dates. Also could mention women\\'s? But ask India won the world cup in cricket. So answer: 1983 and 2011. Provide details.'), TextPart(content='India has lifted the ICC\\u202fCricket World Cup (the 50â€‘over\\u202fODI tournament) **twice**:\\n\\n| Year | Host nation(s) | Final opponent | Margin of victory | Captain |\\n|------|----------------|----------------|-------------------|---------|\\n| **1983** | England | West Indies | 43 runs | **Kapil\\u202fDev** |\\n| **2011** | India, Sri\\u202fLanka & Bangladesh (final in Mumbai) | Sri\\u202fLanka | 6 wickets | **Mahendra\\u202fâ€œMSâ€\\u202fDhoni** |\\n\\n**Key points**\\n\\n- **1983** â€“ India, considered underâ€‘dogs, beat the twoâ€‘time champions West Indies. It was the first World Cup win for any Asian team.  \\n- **2011** â€“ The triumph came 28\\u202fyears later, with Dhoniâ€™s iconic six to finish the chase, giving India its first World Cup win on home soil.\\n\\n*(If youâ€™re also interested in the T20 format, India won the ICC\\u202fMenâ€™s T20 World Cup in 2021 (UAE/\\u200bOman) and the ICC\\u202fWomenâ€™s T20 World Cup in 2023.)*')], usage=RequestUsage(input_tokens=91, output_tokens=362), model_name='openai/gpt-oss-120b', timestamp=datetime.datetime(2025, 12, 9, 5, 8, 20, tzinfo=TzInfo(0)), provider_name='groq', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-93fed20a-c0e4-472b-a7d0-d7d95431eb0c', finish_reason='stop', run_id='118711a7-0130-484c-adcd-d1a1c2ac885f')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await agent.run(\"What was my last question?\", message_history = result.new_messages())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLfCRqKtDpSt",
        "outputId": "d3215661-9906-4c63-f102-5353e9c78bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentRunResult(output='Your most recent question was:\\n\\n**â€œWhen did India won the world cup in cricket?â€**')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent with Structured Response"
      ],
      "metadata": {
        "id": "WXhyl3b9FHYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This example shows how to get structured, type-safe responses from the agent.\n",
        "\n",
        "Key concepts:\n",
        "- Using Pydantic models to define response structure\n",
        "- Type validation and safety\n"
      ],
      "metadata": {
        "id": "AMgpkwToFsI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "model = 'groq:openai/gpt-oss-120b'\n",
        "\n",
        "class ResponseModel(BaseModel):\n",
        "    \"\"\"Structured response with metadata.\"\"\"\n",
        "\n",
        "    response: str\n",
        "    needs_escalation: bool\n",
        "    follow_up_required: bool\n",
        "    sentiment: str = Field(description=\"Customer sentiment analysis\")\n",
        "\n",
        "\n",
        "agent2 = Agent(\n",
        "    model=model,\n",
        "    output_type=ResponseModel,\n",
        "    system_prompt=(\n",
        "        \"You are an intelligent customer support agent. \"\n",
        "        \"Analyze queries carefully and provide structured responses.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "response = await agent2.run(\"How can I track my order #12345?\")\n"
      ],
      "metadata": {
        "id": "PpcVL-XeElxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.output.response)"
      ],
      "metadata": {
        "id": "YvcjYMOVG1z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502d7502-18da-4a3e-cea0-40173e540e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! To track your order #12345, you can:\n",
            "1. Visit our website and log into your account.\n",
            "2. Go to the 'My Orders' section and locate order #12345.\n",
            "3. Click the 'Track Order' button to view the latest shipping status and carrier details.\n",
            "Alternatively, you can use the direct tracking link weâ€™ve sent to your email, or call our support line at 1-800-555-0123 and provide the order number for realâ€‘time updates.\n",
            "If you have any trouble accessing the tracking information, just let us know and weâ€™ll help further.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TmTMpvYf-fMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agentic RAG"
      ],
      "metadata": {
        "id": "7-9Puvi8-svW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "Pt3ug53cBrKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "RS7pTaqGs7Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a retriever tool for RAG\n",
        "\n",
        "def retrieve_docs(query: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieve relevant documents for a query.\n",
        "    In real life, call your vector DB / search index here.\n",
        "    \"\"\"\n",
        "    # TODO: replace this with your real retrieval\n",
        "    fake_corpus = {\n",
        "        \"pydantic\": \"Pydantic is a library for data validation using Python type hints.\",\n",
        "        \"rag\": \"RAG stands for Retrieval Augmented Generation.\",\n",
        "        \"agent\": \"Agents can call tools to fetch external information.\"\n",
        "    }\n",
        "    return [text for key, text in fake_corpus.items() if key in query.lower()]\n"
      ],
      "metadata": {
        "id": "___Vh8pJ-gYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the agent and attach the tool\n",
        "rag_agent = Agent(\n",
        "    \"groq:openai/gpt-oss-120b\",\n",
        "    system_prompt=(\n",
        "        \"You are a RAG assistant.\\n\"\n",
        "        \"- Use the `retrieve_docs` tool whenever user questions may require external info.\\n\"\n",
        "        \"- When you call it, read the returned documents and answer using ONLY that info plus the question.\\n\"\n",
        "        \"- If the tool returns nothing, say you couldn't find anything relevant.\"\n",
        "    ),\n",
        "    tools=[retrieve_docs],\n",
        ")\n"
      ],
      "metadata": {
        "id": "xtEnFGUatG1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Explain what RAG is and how it relates to agents.\""
      ],
      "metadata": {
        "id": "X-9VY-zsvN5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await rag_agent.run(question)\n",
        "result.all_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5d4-AxiuVRO",
        "outputId": "297f6985-63bc-4fb0-edf3-e9f64c15a488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ModelRequest(parts=[SystemPromptPart(content=\"You are a RAG assistant.\\n- Use the `retrieve_docs` tool whenever user questions may require external info.\\n- When you call it, read the returned documents and answer using ONLY that info plus the question.\\n- If the tool returns nothing, say you couldn't find anything relevant.\", timestamp=datetime.datetime(2025, 12, 9, 5, 18, 10, 802835, tzinfo=datetime.timezone.utc)), UserPromptPart(content='Explain what RAG is and how it relates to agents.', timestamp=datetime.datetime(2025, 12, 9, 5, 18, 10, 802847, tzinfo=datetime.timezone.utc))], run_id='9e438838-f54d-457c-95c1-0b5bf0808fb5'),\n",
              " ModelResponse(parts=[ThinkingPart(content='The user asks: \"Explain what RAG is and how it relates to agents.\"\\n\\nWe need to answer. This is general knowledge; we may not need external docs. However the instructions say: \"Use the retrieve_docs tool whenever user questions may require external info.\" This is a conceptual question that we can answer from our own knowledge. But to follow instructions strictly, perhaps we should still retrieve docs about RAG (retrieval-augmented generation) and agents. Let\\'s call retrieve_docs with query \"RAG retrieval augmented generation definition and relation to agents\".'), ToolCallPart(tool_name='retrieve_docs', args='{\"query\":\"RAG retrieval-augmented generation definition and its relation to AI agents\"}', tool_call_id='fc_21e4816f-4c8b-4cf2-9b65-d17ee0a7f715')], usage=RequestUsage(input_tokens=211, output_tokens=152), model_name='openai/gpt-oss-120b', timestamp=datetime.datetime(2025, 12, 9, 5, 18, 11, tzinfo=TzInfo(0)), provider_name='groq', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-3039bba6-dff3-4b93-823e-92e9bba7cfe7', finish_reason='tool_call', run_id='9e438838-f54d-457c-95c1-0b5bf0808fb5'),\n",
              " ModelRequest(parts=[ToolReturnPart(tool_name='retrieve_docs', content=['RAG stands for Retrieval Augmented Generation.', 'Agents can call tools to fetch external information.'], tool_call_id='fc_21e4816f-4c8b-4cf2-9b65-d17ee0a7f715', timestamp=datetime.datetime(2025, 12, 9, 5, 18, 11, 241169, tzinfo=datetime.timezone.utc))], run_id='9e438838-f54d-457c-95c1-0b5bf0808fb5'),\n",
              " ModelResponse(parts=[ThinkingPart(content='We have retrieved docs: two bullet points: \"RAG stands for Retrieval Augmented Generation.\" and \"Agents can call tools to fetch external information.\"\\n\\nWe need to answer: explain what RAG is and how it relates to agents. Use only info from docs plus the question. So we can state definition: Retrieval Augmented Generation, and relation: agents can call tools to fetch external info, which is similar to RAG\\'s retrieval step. Provide explanation.\\n\\n'), TextPart(content='**RAG (Retrievalâ€‘Augmented Generation)** is a technique where a language model first **retrieves** relevant external information and then **generates** its response using that retrieved material.  \\n\\n**Relation to agents:**  \\nAgents are designed to **call tools** (such as search or database lookâ€‘ups) to obtain upâ€‘toâ€‘date or specialized data. This toolâ€‘calling step is essentially the â€œretrievalâ€ part of RAG, and the agentâ€™s subsequent reasoning or response generation is the â€œgenerationâ€ part. In this way, agents implement RAG by fetching external information before producing their answer.')], usage=RequestUsage(input_tokens=269, output_tokens=227), model_name='openai/gpt-oss-120b', timestamp=datetime.datetime(2025, 12, 9, 5, 18, 11, tzinfo=TzInfo(0)), provider_name='groq', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-c3a10299-4b52-44a4-83ae-2d6501b9c43b', finish_reason='stop', run_id='9e438838-f54d-457c-95c1-0b5bf0808fb5')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a RAG-style query\n",
        "\n",
        "async def ask(question: str):\n",
        "    result = await rag_agent.run(question)\n",
        "    print(result.output)\n"
      ],
      "metadata": {
        "id": "DexvM5UNtMNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await ask(question)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R51BUYwDyBIE",
        "outputId": "d9ea568f-9686-452e-ad23-4afe99c01534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG stands for **Retrievalâ€¯Augmentedâ€¯Generation**. Itâ€™s a pattern where a language model first retrieves relevant external information and then uses that information to generate its response.  \n",
            "\n",
            "In the context of AI **agents**, the same idea appears when an agent is given the ability to call tools (such as a search or database lookup) to fetch upâ€‘toâ€‘date or specialized data before producing an answer. The toolâ€‘calling capability of agents is essentially the â€œretrievalâ€ step of RAG, and the subsequent languageâ€‘model generation is the â€œaugmented generationâ€ step. Thus, RAG describes the overall workflow, while agents are the entities that can execute that workflow by invoking retrieval tools and then generating responses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch which tool is called"
      ],
      "metadata": {
        "id": "PPWowUxI38DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai.messages import ModelResponse, ToolCallPart\n",
        "\n",
        "def get_tools_called(result):\n",
        "    tools_called = []\n",
        "\n",
        "    for msg in result.all_messages():\n",
        "        if isinstance(msg, ModelResponse):\n",
        "            for part in msg.parts:\n",
        "                if isinstance(part, ToolCallPart):\n",
        "                    tools_called.append(part.tool_name)\n",
        "\n",
        "    return tools_called\n",
        "\n",
        "get_tools_called(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvxOI8uS1cYD",
        "outputId": "2225a54c-890a-4cc2-8fe7-b4484b300dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['retrieve_docs']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Evaluating a RAG application (retrieval + answer quality)\n",
        "\n",
        "Here we combine:\n",
        "\n",
        "A Pydantic-AI RAG agent using a search_docs tool (very similar to the official Pydantic RAG example).\n",
        "\n",
        "\n",
        "Pydantic-Evals to:\n",
        "\n",
        "1. Check that the answer is grounded in retrieved docs\n",
        "\n",
        "2. Check that the answer is relevant to the question"
      ],
      "metadata": {
        "id": "7jdAda5PsR_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RAG agent skeleton\n",
        "\n",
        "Assume you already have an in-memory document store and a simple retrieval function."
      ],
      "metadata": {
        "id": "xF_o38PRshlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "from pydantic_ai import Agent, RunContext\n",
        "\n",
        "# --- Domain models ----------------------------------------------------\n",
        "\n",
        "class DocChunk(BaseModel):\n",
        "    id: str\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class RAGDeps:\n",
        "    # could be a vector store or just a list of docs in a PoC\n",
        "    documents: List[DocChunk]\n",
        "\n",
        "# --- Retrieval tool ---------------------------------------------------\n",
        "\n",
        "def retrieve_relevant_chunks(query: str, docs: List[DocChunk], k: int = 3) -> List[DocChunk]:\n",
        "    # toy implementation: top-k by simple keyword overlap\n",
        "    scores = []\n",
        "    query_terms = set(query.lower().split())\n",
        "    for d in docs:\n",
        "        overlap = len(query_terms & set(d.text.lower().split()))\n",
        "        scores.append((overlap, d))\n",
        "    scores.sort(reverse=True, key=lambda t: t[0])\n",
        "    return [d for score, d in scores[:k] if score > 0]\n",
        "\n",
        "# --- RAG agent --------------------------------------------------------\n",
        "\n",
        "class RAGAnswer(BaseModel):\n",
        "    answer: str\n",
        "    used_doc_ids: List[str]\n",
        "\n",
        "rag_agent = Agent[RAGDeps, RAGAnswer](\n",
        "    \"openai:gpt-4o-mini\",\n",
        "    deps_type=RAGDeps,\n",
        "    output_type=RAGAnswer,\n",
        "    instructions=\"\"\"\n",
        "    You are a documentation assistant.\n",
        "    Use ONLY the provided context chunks to answer the question.\n",
        "    If the answer is not in the context, say you don't know.\n",
        "    Return the IDs of the chunks you used.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "@rag_agent.tool  # function tool exposes retrieval to the model\n",
        "def search_docs(ctx: RunContext[RAGDeps], query: str) -> List[DocChunk]:\n",
        "    return retrieve_relevant_chunks(query, ctx.deps.documents)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k_q_Mc7d-gQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Task function for evals\n",
        "\n",
        "We want the task function to return both answer text and which docs were used:"
      ],
      "metadata": {
        "id": "rs44lyf4sutH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List\n",
        "\n",
        "class RAGOutput(TypedDict):\n",
        "    answer: str\n",
        "    used_doc_ids: List[str]\n",
        "\n",
        "def rag_task(question: str, deps: RAGDeps) -> RAGOutput:\n",
        "    result = rag_agent.run_sync(question, deps=deps)\n",
        "    out = result.output\n",
        "    return {\"answer\": out.answer, \"used_doc_ids\": out.used_doc_ids}\n"
      ],
      "metadata": {
        "id": "-5_pe1dKCd8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simple evals we can partially â€œfixâ€ dependencies (e.g., same KB for all cases):"
      ],
      "metadata": {
        "id": "OGcKg-vUtepP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Partial application to match the expected signature inputs -> output\n",
        "my_docs = [\n",
        "    DocChunk(id=\"d1\", text=\"Our refund policy allows returns within 30 days.\"),\n",
        "    DocChunk(id=\"d2\", text=\"Tech support is available 24/7 via chat.\"),\n",
        "]\n",
        "\n",
        "def rag_task_fixed(question: str) -> RAGOutput:\n",
        "    return rag_task(question, deps=RAGDeps(documents=my_docs))\n"
      ],
      "metadata": {
        "id": "lSPhwPdXtgMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define RAG-specific eval dataset\n",
        "\n",
        "Key idea: in each case we specify both:\n",
        "\n",
        "1. The question\n",
        "\n",
        "2. The expected supporting doc IDs\n",
        "\n",
        "3. Optionally, a reference answer"
      ],
      "metadata": {
        "id": "9bVrJyhvs6Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_evals import Case, Dataset\n",
        "from pydantic_evals.evaluators import LLMJudge\n",
        "\n",
        "class RAGInputs(BaseModel):\n",
        "    question: str\n",
        "    expected_doc_ids: List[str]\n",
        "    reference_answer: str\n",
        "\n",
        "dataset = Dataset[RAGInputs, RAGOutput](\n",
        "    cases=[\n",
        "        Case(\n",
        "            name=\"refund_policy\",\n",
        "            inputs=RAGInputs(\n",
        "                question=\"What is your refund policy?\",\n",
        "                expected_doc_ids=[\"d1\"],\n",
        "                reference_answer=\"We allow refunds within 30 days of purchase.\",\n",
        "            ),\n",
        "        ),\n",
        "        Case(\n",
        "            name=\"support_hours\",\n",
        "            inputs=RAGInputs(\n",
        "                question=\"When is tech support available?\",\n",
        "                expected_doc_ids=[\"d2\"],\n",
        "                reference_answer=\"Tech support is available 24/7 via chat.\",\n",
        "            ),\n",
        "        ),\n",
        "    ],\n",
        ")\n"
      ],
      "metadata": {
        "id": "JEvHz2J5s_-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Add custom evaluators: retrieval + groundedness\n",
        "\n",
        "You can layer:\n",
        "\n",
        "Retrieval precision: Did the agent use the right doc IDs?\n",
        "\n",
        "Groundedness: Is the answer consistent with provided docs?\n",
        "(Good use case for LLMJudge.)\n"
      ],
      "metadata": {
        "id": "eB9EN8PstFcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Retrieval evaluator (deterministic)\n",
        "\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pydantic_evals.evaluators import Evaluator, EvaluatorContext, EvaluationReason\n",
        "\n",
        "@dataclass\n",
        "class RetrievalMatch(Evaluator[RAGInputs, RAGOutput, None]):\n",
        "    \"\"\"Check that the agent used all expected docs.\"\"\"\n",
        "\n",
        "    def evaluate(self, ctx: EvaluatorContext[RAGInputs, RAGOutput, None]):\n",
        "        expected = set(ctx.inputs.expected_doc_ids)\n",
        "        used = set(ctx.output[\"used_doc_ids\"])\n",
        "        missing = expected - used\n",
        "        extra = used - expected\n",
        "\n",
        "        ok = not missing  # require at least all expected docs\n",
        "        reason = f\"missing={missing}, extra={extra}\"\n",
        "        return EvaluationReason(value=ok, reason=reason)\n"
      ],
      "metadata": {
        "id": "duh2EIC1tM14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attach this evaluator to the dataset:"
      ],
      "metadata": {
        "id": "jhiTrw5Rt2uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.add_evaluator(RetrievalMatch())"
      ],
      "metadata": {
        "id": "kG1F5wXAt3i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(You can also attach evaluators to specific cases by specific_case=\"refund_policy\".)"
      ],
      "metadata": {
        "id": "YFrQgYqBt9TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Grounded answer evaluator with LLMJudge\n",
        "\n",
        "\n",
        "from pydantic_evals.evaluators import LLMJudge\n",
        "\n",
        "groundedness_rubric = \"\"\"\n",
        "You are evaluating an answer produced by a RAG system.\n",
        "\n",
        "Criteria (True vs False):\n",
        "- TRUE if the answer is fully supported by the provided reference_answer text\n",
        "  and does not introduce any contradictions or extra facts.\n",
        "- FALSE if the answer contradicts, fabricates, or goes beyond the reference.\n",
        "\"\"\"\n",
        "\n",
        "dataset.add_evaluator(\n",
        "    LLMJudge(\n",
        "        rubric=groundedness_rubric,\n",
        "        include_input=True,            # see the question + inputs\n",
        "        include_expected_output=True,  # see reference_answer\n",
        "        # model optional â€“ uses default judge model, typically `openai:gpt-4o`\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "### Under the hood, LLMJudge will call a judge model and return a boolean/score with an explanation."
      ],
      "metadata": {
        "id": "Z2dRB5s7t96P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run RAG evaluation"
      ],
      "metadata": {
        "id": "ke0fX--0uKW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = dataset.evaluate_sync(rag_task_fixed)\n",
        "report.print()"
      ],
      "metadata": {
        "id": "Yq6axVNIuKGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Evaluating a Agent with Pydantic-Evals"
      ],
      "metadata": {
        "id": "yOgUccOwulXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Agent we want to evaluate\n",
        "\n",
        "A minimal Pydantic-AI agent that classifies user queries into support intents:"
      ],
      "metadata": {
        "id": "YXeAbkq8utbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from pydantic import BaseModel\n",
        "from pydantic_ai import Agent\n",
        "\n",
        "class IntentOutput(BaseModel):\n",
        "    intent: Literal[\"refund\", \"technical_support\", \"sales\", \"other\"]\n",
        "    reasoning: str\n",
        "\n",
        "# Pydantic AI agent: takes a user query and returns typed output\n",
        "intent_agent = Agent[None, IntentOutput](\n",
        "    \"openai:gpt-4o-mini\",  # or any configured model\n",
        "    instructions=\"\"\"\n",
        "    You are a support triage bot.\n",
        "    Classify the user's message into one of: refund, technical_support, sales, other.\n",
        "    Explain your reasoning in one or two sentences.\n",
        "    \"\"\",\n",
        "    output_type=IntentOutput,\n",
        ")\n"
      ],
      "metadata": {
        "id": "0CNv_qUEusvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is straight out of typical Pydantic-AI usage: agents are parameterized by dependency type and output model, and output_type is a Pydantic model that gets validated\n",
        "\n",
        "\n",
        "Weâ€™ll now evaluate whether the classification is correct across a dataset."
      ],
      "metadata": {
        "id": "1dF_uX2RvNvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Wrap the agent as a task function\n",
        "\n",
        "Pydantic-Evals expects a function to call for each test case. We wrap the agent:"
      ],
      "metadata": {
        "id": "PDYYfSn6vYYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_intent_task(user_message: str) -> str:\n",
        "    \"\"\"Task function for evals â€“ returns only the intent label.\"\"\"\n",
        "    result = intent_agent.run_sync(user_message)\n",
        "    return result.output.intent\n"
      ],
      "metadata": {
        "id": "JypwBdz-vV6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define dataset + evaluators (deterministic + LLM-judge)"
      ],
      "metadata": {
        "id": "l2z2c-bGveNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_evals import Case, Dataset\n",
        "from pydantic_evals.evaluators import EqualsExpected, LLMJudge\n",
        "\n",
        "# 1) Dataset with labeled cases\n",
        "dataset = Dataset[str, str](\n",
        "    cases=[\n",
        "        Case(\n",
        "            name=\"refund_request\",\n",
        "            inputs=\"I want a refund for the headphones I bought last week.\",\n",
        "            expected_output=\"refund\",\n",
        "        ),\n",
        "        Case(\n",
        "            name=\"login_issue\",\n",
        "            inputs=\"I can't log into my account, it keeps saying password invalid.\",\n",
        "            expected_output=\"technical_support\",\n",
        "        ),\n",
        "        Case(\n",
        "            name=\"pricing_question\",\n",
        "            inputs=\"Do you offer any discount if we buy 100 licenses?\",\n",
        "            expected_output=\"sales\",\n",
        "        ),\n",
        "    ],\n",
        "    evaluators=[\n",
        "        # Exact correctness: output == expected_output\n",
        "        EqualsExpected(),\n",
        "        # Subjective check: was the classification reasonable?\n",
        "        LLMJudge(\n",
        "            rubric=(\n",
        "                \"Judge if the predicted intent label is a reasonable \"\n",
        "                \"classification for the given user message. \"\n",
        "                \"Return true only if it clearly fits.\"\n",
        "            ),\n",
        "            include_input=True,\n",
        "            include_expected_output=True,\n",
        "            # model optional â€“ uses default judge model if omitted\n",
        "        ),\n",
        "    ],\n",
        ")\n"
      ],
      "metadata": {
        "id": "KPW6-JhGvfq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- EqualsExpected is a built-in comparison evaluator.\n",
        "\n",
        "\n",
        "- LLMJudge is an LLM-as-a-judge evaluator for subjective criteria (correctness of label given context)."
      ],
      "metadata": {
        "id": "DH3pj1DHvkoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Run the evaluation\n",
        "\n",
        "\n",
        "report = dataset.evaluate_sync(classify_intent_task)\n",
        "\n",
        "# Pretty print high-level summary in training session:\n",
        "report.print()\n"
      ],
      "metadata": {
        "id": "5YFq_ZZyvpSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pydantic-Evalsâ€™ data model is:\n",
        "\n",
        "Dataset â€“ list of Cases + Evaluators\n",
        "\n",
        "Experiment â€“ running dataset.evaluate(task)\n",
        "\n",
        "EvaluationReport â€“ structured result with scores, assertions, durations, etc."
      ],
      "metadata": {
        "id": "QAtTkwXAvutH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing Evaluations on AI Agent using library called as DeepEval"
      ],
      "metadata": {
        "id": "GangluYBm2De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show deepeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lxYaPXdmzuz",
        "outputId": "a6bae484-5b25-4eb9-fcd6-cd63ecbd1c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: deepeval\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wo1t0NeEm1QO",
        "outputId": "1af8b4b2-121a-49ce-cf15-148f6b2e1409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepeval in /usr/local/lib/python3.12/dist-packages (3.7.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from deepeval) (3.13.2)\n",
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.75.0)\n",
            "Requirement already satisfied: click<8.3.0,>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (8.2.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.53.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.76.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from deepeval) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.6.0)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.6.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.8.1)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.39.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from deepeval) (3.2.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=5.4.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (5.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.12.3)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.12.0)\n",
            "Requirement already satisfied: pyfiglet in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.0.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from deepeval) (8.4.2)\n",
            "Requirement already satisfied: pytest-asyncio in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.3.0)\n",
            "Requirement already satisfied: pytest-repeat in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.9.4)\n",
            "Requirement already satisfied: pytest-rerunfailures in /usr/local/lib/python3.12/dist-packages (from deepeval) (16.1)\n",
            "Requirement already satisfied: pytest-xdist in /usr/local/lib/python3.12/dist-packages (from deepeval) (3.8.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.2.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.32.4)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.6.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (13.9.4)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.47.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from deepeval) (75.2.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.9.0)\n",
            "Requirement already satisfied: tenacity<=10.0.0,>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (9.1.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (4.67.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9 in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.20.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.45.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.12.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (2.43.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.39.0)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.39.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (5.29.5)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (0.60b0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (2.9.0.post0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2025.11.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.6.0->deepeval) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.6.0->deepeval) (2.19.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (1.22.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic->deepeval) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->deepeval) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic->deepeval) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->deepeval) (3.0.3)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->deepeval) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from pytest->deepeval) (25.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->deepeval) (1.6.0)\n",
            "Requirement already satisfied: execnet>=2.1 in /usr/local/lib/python3.12/dist-packages (from pytest-xdist->deepeval) (2.1.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.6.0->deepeval) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuring deepeval"
      ],
      "metadata": {
        "id": "ucS1DXehpiBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Login to Confident AI\n",
        "\n",
        "import deepeval\n",
        "deepeval.login(\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KyoYKjKAm1NX",
        "outputId": "dcd98251-7b92-42a5-91ea-fa9cab7c491e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸ‰ğŸ¥³ Congratulations! You've successfully logged in! ğŸ™Œ \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ‰ğŸ¥³ Congratulations! You've successfully logged in! ğŸ™Œ \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### if you want to use Groq API as you LLM provider for Deepeval then please perform the below configuation\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.groq.com/openai/v1\"  # Groqâ€™s OpenAI-compatible API\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"               # use your Groq key here"
      ],
      "metadata": {
        "id": "gVh0ENieqQG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Set Groq model as local model in Deepeval\n",
        "\n",
        "!deepeval set-local-model --model-name=\"openai/gpt-oss-120b\" --base-url=\"https://api.groq.com/openai/v1\" --api-key=\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqyhD-o_m1I4",
        "outputId": "02834f1a-93e4-42fb-91e8-3cec173fa3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings updated for this session. To persist, use --\u001b[33msave\u001b[0m=\u001b[35mdotenv\u001b[0m\u001b[1m[\u001b[0m:path\u001b[1m]\u001b[0m \u001b[1m(\u001b[0mdefault\n",
            ".env.local\u001b[1m)\u001b[0m or set \u001b[33mDEEPEVAL_DEFAULT_SAVE\u001b[0m=\u001b[35mdotenv\u001b[0m:.env.local\n",
            "ğŸ™Œ Congratulations! You're now using a local model `openai/gpt-oss-120b` for all\n",
            "evals that require an LLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing AI Agent"
      ],
      "metadata": {
        "id": "nr7GL8dfu0eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a test case - Here we are checking weather the correct tool was called for a giver user question\n",
        "\n",
        "\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.test_case import ToolCall\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "\n",
        "    input=question,             ### User question\n",
        "\n",
        "    tools_called=[ToolCall(name=get_tools_called(result)[-1])],                ### Tool called by the agent\n",
        "\n",
        "    actual_output=result.output,    ### Agent response\n",
        "\n",
        "    expected_output=result.output,  ### Ground truth answer\n",
        "\n",
        "    expected_tools=[ToolCall(name = 'retrieve_docs')] ###Expected tool to be called Ground truth\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "id": "8gNu2S1Pm1E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_case.input)\n",
        "\n",
        "print(test_case.actual_output)\n",
        "\n",
        "print(test_case.expected_output)\n",
        "\n",
        "print(test_case.tools_called)\n",
        "\n",
        "print(test_case.expected_tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4pqjMEbm1BF",
        "outputId": "f6eddadd-ae36-47cd-9ee6-1ba3cf54581f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain what RAG is and how it relates to agents.\n",
            "**RAG (Retrievalâ€‘Augmented Generation)** is a technique where a language model first **retrieves** relevant external information and then **generates** its response using that retrieved material.  \n",
            "\n",
            "**Relation to agents:**  \n",
            "Agents are designed to **call tools** (such as search or database lookâ€‘ups) to obtain upâ€‘toâ€‘date or specialized data. This toolâ€‘calling step is essentially the â€œretrievalâ€ part of RAG, and the agentâ€™s subsequent reasoning or response generation is the â€œgenerationâ€ part. In this way, agents implement RAG by fetching external information before producing their answer.\n",
            "**RAG (Retrievalâ€‘Augmented Generation)** is a technique where a language model first **retrieves** relevant external information and then **generates** its response using that retrieved material.  \n",
            "\n",
            "**Relation to agents:**  \n",
            "Agents are designed to **call tools** (such as search or database lookâ€‘ups) to obtain upâ€‘toâ€‘date or specialized data. This toolâ€‘calling step is essentially the â€œretrievalâ€ part of RAG, and the agentâ€™s subsequent reasoning or response generation is the â€œgenerationâ€ part. In this way, agents implement RAG by fetching external information before producing their answer.\n",
            "[ToolCall(\n",
            "    name=\"retrieve_docs\"\n",
            ")]\n",
            "[ToolCall(\n",
            "    name=\"retrieve_docs\"\n",
            ")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Create an evaluation dataset\n",
        "import deepeval\n",
        "from deepeval.dataset import EvaluationDataset"
      ],
      "metadata": {
        "id": "UfnyRdo8m04u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Add the test case to the evaluation dataset\n",
        "dataset = EvaluationDataset()\n",
        "dataset.add_test_case(test_case)"
      ],
      "metadata": {
        "id": "J-DXzFHom0se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzyCkh-T5kL6",
        "outputId": "1a9e420e-b5d6-4e6d-8a0e-49b93f530439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationDataset(test_cases=[LLMTestCase(input='Explain what RAG is and how it relates to agents.', actual_output='**RAG (Retrievalâ€‘Augmented Generation)** is a technique where a language model first **retrieves** relevant external information and then **generates** its response using that retrieved material.  \\n\\n**Relation to agents:**  \\nAgents are designed to **call tools** (such as search or database lookâ€‘ups) to obtain upâ€‘toâ€‘date or specialized data. This toolâ€‘calling step is essentially the â€œretrievalâ€ part of RAG, and the agentâ€™s subsequent reasoning or response generation is the â€œgenerationâ€ part. In this way, agents implement RAG by fetching external information before producing their answer.', expected_output='**RAG (Retrievalâ€‘Augmented Generation)** is a technique where a language model first **retrieves** relevant external information and then **generates** its response using that retrieved material.  \\n\\n**Relation to agents:**  \\nAgents are designed to **call tools** (such as search or database lookâ€‘ups) to obtain upâ€‘toâ€‘date or specialized data. This toolâ€‘calling step is essentially the â€œretrievalâ€ part of RAG, and the agentâ€™s subsequent reasoning or response generation is the â€œgenerationâ€ part. In this way, agents implement RAG by fetching external information before producing their answer.', context=None, retrieval_context=None, additional_metadata=None, tools_called=[ToolCall(\n",
              "    name=\"retrieve_docs\"\n",
              ")], comments=None, expected_tools=[ToolCall(\n",
              "    name=\"retrieve_docs\"\n",
              ")], token_cost=None, completion_time=None, name=None, tags=None, mcp_servers=None, mcp_tools_called=None, mcp_resources_called=None, mcp_prompts_called=None)], goldens=[], _alias=None, _id=None, _multi_turn=False)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.goldens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCQP_zjf5mQx",
        "outputId": "163a160c-be36-433b-9669-ea75de9473e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### We want to evaluate the tool correctness metric on this test case\n",
        "\n",
        "from deepeval.metrics import ToolCorrectnessMetric"
      ],
      "metadata": {
        "id": "KVBc30eE5org"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LOCAL_MODEL_API_KEY\"] = os.environ[\"OPENAI_API_KEY\"]\n",
        "os.environ[\"LOCAL_MODEL_BASE_URL\"] = \"https://api.groq.com/openai/v1\"\n",
        "os.environ[\"LOCAL_MODEL_NAME\"] = \"openai/gpt-oss-120b\""
      ],
      "metadata": {
        "id": "1yEzQTpl686_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate testcases w.r.t predefined metrics ToolCorrectnessMetric()\n",
        "\n",
        "deepeval.evaluate(dataset.test_cases, metrics= [ToolCorrectnessMetric()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4e655c6fff194322b4ec1abba16afa99",
            "e20229f8022c4b73a5eab001fa34b0d8"
          ]
        },
        "id": "Q1xuxLXM5rCW",
        "outputId": "66084509-ae0c-4c59-ecab-71e41988b75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mTool Correctness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Tool Correctness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e655c6fff194322b4ec1abba16afa99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:deepeval.evaluate.execute:in _a_execute_llm_test_cases\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - âœ… Tool Correctness (score: 1.0, threshold: 0.5, strict: False, evaluation model: None, reason: [\n",
            "\t Tool Calling Reason: All expected tools ['retrieve_docs'] were called (order not considered).\n",
            "\t Tool Selection Reason: No available tools were provided to assess tool selection criteria\n",
            "]\n",
            ", error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Explain what RAG is and how it relates to agents.\n",
            "  - actual output: **RAG (Retrievalâ€‘Augmented Generation)** is a technique where a language model first **retrieves** relevant external information and then **generates** its response using that retrieved material.  \n",
            "\n",
            "**Relation to agents:**  \n",
            "Agents are designed to **call tools** (such as search or database lookâ€‘ups) to obtain upâ€‘toâ€‘date or specialized data. This toolâ€‘calling step is essentially the â€œretrievalâ€ part of RAG, and the agentâ€™s subsequent reasoning or response generation is the â€œgenerationâ€ part. In this way, agents implement RAG by fetching external information before producing their answer.\n",
            "  - expected output: **RAG (Retrievalâ€‘Augmented Generation)** is a technique where a language model first **retrieves** relevant external information and then **generates** its response using that retrieved material.  \n",
            "\n",
            "**Relation to agents:**  \n",
            "Agents are designed to **call tools** (such as search or database lookâ€‘ups) to obtain upâ€‘toâ€‘date or specialized data. This toolâ€‘calling step is essentially the â€œretrievalâ€ part of RAG, and the agentâ€™s subsequent reasoning or response generation is the â€œgenerationâ€ part. In this way, agents implement RAG by fetching external information before producing their answer.\n",
            "  - context: None\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Tool Correctness: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33mâš  WARNING:\u001b[0m No hyperparameters logged.\n",
              "Â» \u001b]8;id=660831;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
              "\n",
              "================================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">âš  WARNING:</span> No hyperparameters logged.\n",
              "Â» <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
              "\n",
              "================================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141mâœ“\u001b[0m Done ğŸ‰! View results on \n",
              "\u001b]8;id=3159;https://app.confident-ai.com/project/cmix2a74i00pvpm1fyu5zhour/test-runs/cmiy8rubb0juakc1f8kfmln5f/regression-testing\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmix2a74i00pvpm1fyu5zhour/test-runs/cmiy8rubb0juakc1f8kfmln5f/regression-testi\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b]8;id=3159;https://app.confident-ai.com/project/cmix2a74i00pvpm1fyu5zhour/test-runs/cmiy8rubb0juakc1f8kfmln5f/regression-testing\u001b\\\u001b[4;94mng\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Done ğŸ‰! View results on \n",
              "<a href=\"https://app.confident-ai.com/project/cmix2a74i00pvpm1fyu5zhour/test-runs/cmiy8rubb0juakc1f8kfmln5f/regression-testing\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmix2a74i00pvpm1fyu5zhour/test-runs/cmiy8rubb0juakc1f8kfmln5f/regression-testi</span></a>\n",
              "<a href=\"https://app.confident-ai.com/project/cmix2a74i00pvpm1fyu5zhour/test-runs/cmiy8rubb0juakc1f8kfmln5f/regression-testing\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ng</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Tool Correctness', threshold=0.5, success=True, score=1.0, reason=\"[\\n\\t Tool Calling Reason: All expected tools ['retrieve_docs'] were called (order not considered).\\n\\t Tool Selection Reason: No available tools were provided to assess tool selection criteria\\n]\\n\", strict_mode=False, evaluation_model=None, error=None, evaluation_cost=0.0, verbose_logs='Expected Tools:\\n[\\n    ToolCall(\\n        name=\"retrieve_docs\"\\n    )\\n] \\n \\nTools Called:\\n[\\n    ToolCall(\\n        name=\"retrieve_docs\"\\n    )\\n] \\n \\nAvailable Tools: [] \\n \\nTool Selection Score: 1.0 \\n \\nTool Selection Reason: No available tools were provided to assess tool selection criteria')], conversational=False, multimodal=False, input='Explain what RAG is and how it relates to agents.', actual_output='**RAG (Retrievalâ€‘Augmented Generation)** is a technique where a language model first **retrieves** relevant external information and then **generates** its response using that retrieved material.  \\n\\n**Relation to agents:**  \\nAgents are designed to **call tools** (such as search or database lookâ€‘ups) to obtain upâ€‘toâ€‘date or specialized data. This toolâ€‘calling step is essentially the â€œretrievalâ€ part of RAG, and the agentâ€™s subsequent reasoning or response generation is the â€œgenerationâ€ part. In this way, agents implement RAG by fetching external information before producing their answer.', expected_output='**RAG (Retrievalâ€‘Augmented Generation)** is a technique where a language model first **retrieves** relevant external information and then **generates** its response using that retrieved material.  \\n\\n**Relation to agents:**  \\nAgents are designed to **call tools** (such as search or database lookâ€‘ups) to obtain upâ€‘toâ€‘date or specialized data. This toolâ€‘calling step is essentially the â€œretrievalâ€ part of RAG, and the agentâ€™s subsequent reasoning or response generation is the â€œgenerationâ€ part. In this way, agents implement RAG by fetching external information before producing their answer.', context=None, retrieval_context=None, turns=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmix2a74i00pvpm1fyu5zhour/test-runs/cmiy8rubb0juakc1f8kfmln5f/regression-testing', test_run_id='cmiy8rubb0juakc1f8kfmln5f')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}